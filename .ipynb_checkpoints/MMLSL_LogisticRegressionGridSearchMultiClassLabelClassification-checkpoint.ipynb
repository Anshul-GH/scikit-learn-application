{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'vect__max_features': (2500, 5000, 10000, None),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__use_idf': (True, False),\n",
    "    'vect__norm': ('l1', 'l2'),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__C': (0.01, 0.1, 1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/anshul/MMLSL/chapter06/SMSSpamCollection', delimiter='\\t', header=None)\n",
    "X = df[1].values\n",
    "y = df[0].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1536 candidates, totalling 4608 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 876 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1322 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1772 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2322 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2972 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3722 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4572 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.25, 0.5, 0.75), 'vect__stop_words': ('english', None), 'vect__max_features': (2500, 5000, 10000, None), 'vect__ngram_range': ((1, 1), (1, 2)), 'vect__use_idf': (True, False), 'vect__norm': ('l1', 'l2'), 'clf__penalty': ('l1', 'l2'), 'clf__C': (0.01, 0.1, 1, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.985\n",
      "\n",
      "Best Parameter Set: \n",
      "clf__C: 10\n",
      "clf__penalty: l2\n",
      "vect__max_df: 0.25\n",
      "vect__max_features: 5000\n",
      "vect__ngram_range: (1, 2)\n",
      "vect__norm: l2\n",
      "vect__stop_words: None\n",
      "vect__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: {0:.3f}'.format(grid_search.best_score_))\n",
    "print('\\nBest Parameter Set: ')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('{}: {}'.format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.988\n",
      "Precision: 0.994\n",
      "Recall: 0.914\n",
      "F1-Score: 0.953\n"
     ]
    }
   ],
   "source": [
    "predictions = grid_search.predict(X_test)\n",
    "\n",
    "print('Accuracy: {0:.3f}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {0:.3f}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {0:.3f}'.format(recall_score(y_test, predictions)))\n",
    "print('F1-Score: {0:.3f}'.format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseId      156060\n",
      "SentenceId    156060\n",
      "Phrase        156060\n",
      "Sentiment     156060\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# one-vs-all classification\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/anshul/MMLSL/chapter06/train.tsv', delimiter='\\t', header=0)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    A series of escapades demonstrating the adage ...\n",
      "1    A series of escapades demonstrating the adage ...\n",
      "2                                             A series\n",
      "3                                                    A\n",
      "4                                               series\n",
      "5    of escapades demonstrating the adage that what...\n",
      "6                                                   of\n",
      "7    escapades demonstrating the adage that what is...\n",
      "8                                            escapades\n",
      "9    demonstrating the adage that what is good for ...\n",
      "Name: Phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Phrase'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    156060.000000\n",
      "mean          2.063578\n",
      "std           0.893832\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max           4.000000\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examining the target/response variable - Sentiment\n",
    "print(df['Sentiment'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    50.994489\n",
      "3    21.098936\n",
      "1    17.475971\n",
      "4     5.899013\n",
      "0     4.531590\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print('{0:.2f}'.format((df['Sentiment'].value_counts() / df['Sentiment'].count()) * 100))\n",
    "print((df['Sentiment'].value_counts() / df['Sentiment'].count()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anshul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# training the classifier with scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('/home/anshul/MMLSL/chapter06/train.tsv', delimiter='\\t', header=0)\n",
    "X = df['Phrase'] \n",
    "y = df['Sentiment'].as_matrix()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.5)\n",
    "\n",
    "# grid_search = main(X_train, y_train)\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.25, 0.5),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__use_idf': (True, False),\n",
    "    'clf__C': (0.1, 1, 10),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.25, 0.5), 'vect__ngram_range': ((1, 1), (1, 2)), 'vect__use_idf': (True, False), 'clf__C': (0.1, 1, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.620\n",
      "\n",
      "Best Parameter Set: \n",
      "clf__C: 10\n",
      "vect__max_df: 0.25\n",
      "vect__ngram_range: (1, 2)\n",
      "vect__use_idf: False\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: {0:.3f}'.format(grid_search.best_score_))\n",
    "print('\\nBest Parameter Set: ')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('{}: {}'.format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.635768294245803\n",
      "Confusion Matrix:\n",
      "[[ 1133  1759   604    61     8]\n",
      " [  922  6021  6170   540    33]\n",
      " [  203  3188 32601  3584   162]\n",
      " [   21   406  6535  8211  1303]\n",
      " [    4    38   521  2359  1643]]\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.32      0.39      3565\n",
      "          1       0.53      0.44      0.48     13686\n",
      "          2       0.70      0.82      0.76     39738\n",
      "          3       0.56      0.50      0.53     16476\n",
      "          4       0.52      0.36      0.43      4565\n",
      "\n",
      "avg / total       0.62      0.64      0.62     78030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = grid_search.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
