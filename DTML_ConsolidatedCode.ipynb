{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidated Executable Code from DTML_MasterImplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'MONTH', 'QUARTER', 'WEEK', 'WEEKDAY', 'WEEK_OF_YEAR', 'YEAR', 'LOC', 'CRFA_R', 'CRFA_F', 'CRFA_A', 'EOQ', 'PKG_QTY', 'SCORE_862_INSTABILITY_AVG', 'AVG_ADJ_QTY', 'DESTINID', 'AVG_DAYS_LATE']\n",
      "Count of features before one-hot-encoding:  22\n",
      "Count of features after one-hot-encoding:  22\n",
      "X_train Type:  <class 'pandas.core.frame.DataFrame'>\n",
      "X_test Type:  <class 'pandas.core.frame.DataFrame'>\n",
      "y_train Type:  <class 'pandas.core.series.Series'>\n",
      "y_test Type:  <class 'pandas.core.series.Series'>\n",
      "X_train Shape:  (77589, 22)\n",
      "X_test Shape:  (25864, 22)\n",
      "y_train Shape:  (77589,)\n",
      "y_test Shape:  (25864,)\n",
      "y_trbin Type:  <class 'numpy.ndarray'>\n",
      "y_tsbin Type:  <class 'numpy.ndarray'>\n",
      "y_trbin Type:  <class 'pandas.core.series.Series'>\n",
      "y_trbin Shape:  (77589,)\n",
      "y_tsbin Type:  <class 'pandas.core.series.Series'>\n",
      "y_tsbin Shape:  (25864,)\n",
      "Accuracy:  0.2992189916486236\n",
      "Recall:  0.2992189916486236\n",
      "Precision:  1.0\n",
      "F1-Score:  0.4606136356872898\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Short       1.00      0.30      0.46     25864\n",
      "\n",
      "avg / total       1.00      0.30      0.46     25864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# read the data from the csv file\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\sp3Master.csv\")\n",
    "data.head()\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# everything other than the SR_FLAG is feature vector\n",
    "# the SR_FLAG is the response variable\n",
    "\n",
    "# assigning 'SR_FLAG' to the response vector 'y'\n",
    "y = data['SR_FLAG']\n",
    "\n",
    "# Using the dataframe.drop() function from the pandas library to \n",
    "# remove one column from the data source and create the feature vector set.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "# the first parameter to the function drop is the name of the column \n",
    "# and the second parameter 'axis' indicates whether we are referring to an index or an actual column name:\n",
    "# 0 or ‘index’, 1 or ‘columns’\n",
    "X = data.drop('SR_FLAG', axis='columns')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# split CAL_DATE to numeric values\n",
    "## SUPER IMPT: This code block only executes once. To reexecute, reset the value of X to default again ##\n",
    "\n",
    "# convert CAL_DATE to datetime type\n",
    "X['CAL_DATE'] = pd.to_datetime(X['CAL_DATE'])\n",
    "\n",
    "# from X, split the date column to individual columns and append them back to X\n",
    "X_ext = pd.DataFrame({\n",
    "    # 'CAL_DATE': X['CAL_DATE'],\n",
    "    'YEAR': X['CAL_DATE'].dt.year,\n",
    "    'MONTH': X['CAL_DATE'].dt.month,\n",
    "    'DAY': X['CAL_DATE'].dt.day,\n",
    "    'DAY_OF_YEAR': X['CAL_DATE'].dt.dayofyear,\n",
    "    'WEEK': X['CAL_DATE'].dt.week,\n",
    "    'WEEK_OF_YEAR': X['CAL_DATE'].dt.weekofyear,\n",
    "    'DAY_OF_WEEK': X['CAL_DATE'].dt.dayofweek,\n",
    "    'WEEKDAY': X['CAL_DATE'].dt.weekday,\n",
    "    'QUARTER': X['CAL_DATE'].dt.quarter\n",
    "}).join(X)\n",
    "# .join(X, lsuffix='_left')\n",
    "# }).join(X, on='CAL_DATE', how='right', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# syntax example for join\n",
    "# df_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# drop the original CAL_DATE column\n",
    "X_ext = X_ext.drop('CAL_DATE', axis='columns')\n",
    "\n",
    "# reassign extended X back to X to overrite\n",
    "X = X_ext\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "## CRFA_C\n",
    "# replace the null values within the column 'CRFA_C' with ASCII for '0' that is NUL.\n",
    "X['CRFA_C'].fillna(chr(0), inplace=True)\n",
    "\n",
    "# convert all the string values to ASCII equivalents\n",
    "X['CRFA_C'] = X['CRFA_C'].apply(lambda x: ord(x))\n",
    "\n",
    "\n",
    "##OUTPROC_FLAG\n",
    "# convert the boolean values True/False to integer 1/0\n",
    "X['OUTPROC_FLAG']=X['OUTPROC_FLAG'].apply(lambda x: np.where(x==False,0,1))\n",
    "\n",
    "## hence the only non-numeric column that is left now is ITEM_NO\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# collect all the numeric features in a temp dataframe\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html\n",
    "X_numeric = X.select_dtypes(include=np.number)\n",
    "\n",
    "# fetch the column names of the numeric columns\n",
    "X_numcols = list(X_numeric)\n",
    "print(X_numcols)\n",
    "# use the apply function again to apply logorithmic converion to all numeric columns\n",
    "# since log of negative values is not defined, we validate and change the function for negative values\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: (np.log(1 + x) if x > 0 else -np.log(1 - x)))\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: np.log(1 + x))\n",
    "\n",
    "X[X_numcols] = X[X_numcols].apply(lambda x: ((1+x)/(1+abs(x)))*(np.log(1 + abs(x))))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# Again, scaling only applies to numerical data.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# initializing the scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# define a new dataframe to hold the scaled numeric features\n",
    "X_scaled = pd.DataFrame(data = X)\n",
    "\n",
    "# apply the scaling and populate the data\n",
    "X_scaled[X_numcols] = scaler.fit_transform(X[X_numcols])\n",
    "\n",
    "X_scaled.head()\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# The 'pandas.get_dummies' is a function to convert string columns into one-hot representation.\n",
    "# Alternative is sklearn.preprocessing.OneHotEncoder()\n",
    "# https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-is-more-efficient\n",
    "\n",
    "# count of features before applying one-hot encoding:\n",
    "print('Count of features before one-hot-encoding: ', len(X_scaled.columns))\n",
    "\n",
    "# applying one-hot-encoding\n",
    "# One-hot encoding is failing the modle fitment\n",
    "X_scaled = pd.get_dummies(data=X_scaled)\n",
    "\n",
    "# count of features after applying one-hot encoding:\n",
    "print('Count of features after one-hot-encoding: ', len(X_scaled.columns))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# split the scaled dataset into a training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data in to groups of 75/25 as train/test ratio with a fixed random_state of 0.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# check the types of the records\n",
    "print('X_train Type: ', type(X_train))\n",
    "print('X_test Type: ', type(X_test))\n",
    "print('y_train Type: ', type(y_train))\n",
    "print('y_test Type: ', type(y_test))\n",
    "\n",
    "# check the shapes of the records\n",
    "print('X_train Shape: ', X_train.shape)\n",
    "print('X_test Shape: ', X_test.shape)\n",
    "print('y_train Shape: ', y_train.shape)\n",
    "print('y_test Shape: ', y_test.shape)\n",
    "\n",
    "# Using LabelBinarizer to convert y_train and y_test to a binary array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_trbin = lb.fit_transform(y_train)\n",
    "y_tsbin = lb.fit_transform(y_test)\n",
    "print('y_trbin Type: ', type(y_trbin))\n",
    "print('y_tsbin Type: ', type(y_tsbin))\n",
    "\n",
    "# convert the numpy array back to the pandas series\n",
    "y_trbin = pd.Series(y_trbin.reshape(-1))\n",
    "y_tsbin = pd.Series(y_tsbin.reshape(-1))\n",
    "print('y_trbin Type: ', type(y_trbin))\n",
    "print('y_trbin Shape: ', y_trbin.shape)\n",
    "print('y_tsbin Type: ', type(y_tsbin))\n",
    "print('y_tsbin Shape: ', y_tsbin.shape)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# Calculating the naive parameters for the data\n",
    "# Pre-calculating the metrices for a naive model. \n",
    "# That is when the outcome is always predicted as true, how will the raw model behave.\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, matthews_corrcoef, classification_report\n",
    "\n",
    "# predicting everything as True(1)\n",
    "y_naive = [1]*len(y_test)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_naive, y_test))\n",
    "print('Recall: ', recall_score(y_naive, y_test))\n",
    "print('Precision: ', precision_score(y_naive, y_test))\n",
    "print('F1-Score: ', f1_score(y_naive, y_test))\n",
    "# print('Matthews Correlation Coefficient: ', matthews_corrcoef(y_naive, y_test))\n",
    "print('Classification Report: \\n', classification_report(y_naive, y_test, target_names=['Short'], labels=[1]))\n",
    "# print (\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, fbeta_score\n",
    "\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "## GridSearchCV is throwing a MemoryError ##\n",
    "\n",
    "# attributes for GridSearchCV\n",
    "# parameters for the attribute 'param_grid' ## BALCK BOX ##\n",
    "# parameters = {'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced']}\n",
    "# parameters = {'C': [10], 'class_weight': [None, 'balanced']}\n",
    "# parameters = {'C': [10]}\n",
    "# scorer for the attribute 'scoring' ## BALCK BOX ##\n",
    "# scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# create an object for GridSearchCV\n",
    "# grid_obj = GridSearchCV(estimator=clf, param_grid=parameters, scoring=scorer)\n",
    "\n",
    "# fitting the training dataset\n",
    "# grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# get the best estimator\n",
    "# best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# default prediction - without considering the recommendations from GridSearchCV\n",
    "# fitting the model\n",
    "# clf.fit(X_train, y_train)\n",
    "# making predictions\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# predictions based on the recommendations via GridSearchCV\n",
    "# best_y_pred = best_clf.predict(X_test)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITEM_NO', 'OUTPROC_FLAG']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nl_temp = ['S', '2', '3', 'K', 'D', 'G', 'B', 'E', 'H', 'Z', 'F', nan]\\nl_temp2 = [0]*len(l_temp)\\nfor i in range(len(l_temp)):\\n    if len(str(l_temp[i])) != 1:\\n        l_temp2[i] = 0 #if l_temp[i].isalpha() else ord(chr(l_temp[i]))\\n    else:\\n        l_temp2[i] = ord(l_temp[i])\\nprint(l_temp2)\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the non numeric features\n",
    "# collect all the numeric features in a temp dataframe\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html\n",
    "# X_nonnum = X_scaled.select_dtypes(exclude=np.number)\n",
    "# fetch the column names of the numeric columns\n",
    "# X_nnumcols = list(X_nonnum)\n",
    "# print(X_nnumcols)\n",
    "# use the apply function again to apply logorithmic converion to all numeric columns\n",
    "# since log of negative values is not defined, we validate and change the function for negative values\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: (np.log(1 + x) if x > 0 else -np.log(1 - x)))\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: np.log(1 + x))\n",
    "\n",
    "# use DictVectorizer to convert these colums to numeric fields\n",
    "# check unique CRFA_C counts\n",
    "# X_uniq = X_scaled.groupby('OUTPROC_FLAG').nunique()\n",
    "# print(X_uniq)\n",
    "\n",
    "# convert 'CRFA_C' to ASCII values using the Python ord() function\n",
    "# X_scaled['CRFA_C'] = X_scaled['CRFA_C'].apply(lambda x: ord(x))\n",
    "\n",
    "X_nonnum = X_scaled.select_dtypes(exclude=np.number)\n",
    "X_nnumcols = list(X_nonnum)\n",
    "# print(X_nnumcols)\n",
    "\n",
    "\n",
    "# X_log_scaled['OUTPROC_FLAG']=X_log_scaled['OUTPROC_FLAG'].apply(lambda x: np.where(x==False,0,1))\n",
    "# ord('S')\n",
    "\n",
    "# X_scaled['CRFA_C'].fillna('0', inplace=True)\n",
    "print(X_scaled['CRFA_C'].unique())\n",
    "'''\n",
    "l_temp = ['S', '2', '3', 'K', 'D', 'G', 'B', 'E', 'H', 'Z', 'F', nan]\n",
    "l_temp2 = [0]*len(l_temp)\n",
    "for i in range(len(l_temp)):\n",
    "    if len(str(l_temp[i])) != 1:\n",
    "        l_temp2[i] = 0 #if l_temp[i].isalpha() else ord(chr(l_temp[i]))\n",
    "    else:\n",
    "        l_temp2[i] = ord(l_temp[i])\n",
    "print(l_temp2)\n",
    "'''\n",
    "# X_scaled['CRFA_C'] = X_scaled['CRFA_C'].apply(lambda x: np.where(x.isalpha(), ord(x), ord(chr(x))))\n",
    "# X_scaled = X_scaled[X_scaled['CRFA_C'].notnull()]\n",
    "# X_scaled['CRFA_C'] = X_scaled['CRFA_C'].apply(lambda x: ord(x))\n",
    "\n",
    "# X_scaled['CRFA_C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'MONTH', 'QUARTER', 'WEEK', 'WEEKDAY', 'WEEK_OF_YEAR', 'YEAR', 'LOC', 'CRFA_C', 'CRFA_R', 'CRFA_F', 'CRFA_A', 'EOQ', 'PKG_QTY', 'OUTPROC_FLAG', 'SCORE_862_INSTABILITY_AVG', 'AVG_ADJ_QTY', 'DESTINID', 'AVG_DAYS_LATE']\n",
      "Count of features before one-hot-encoding:  22\n",
      "Count of features after one-hot-encoding:  11421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# read the data from the csv file\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\sp3Master.csv\")\n",
    "data.head()\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# everything other than the SR_FLAG is feature vector\n",
    "# the SR_FLAG is the response variable\n",
    "\n",
    "# assigning 'SR_FLAG' to the response vector 'y'\n",
    "y = data['SR_FLAG']\n",
    "\n",
    "# Using the dataframe.drop() function from the pandas library to \n",
    "# remove one column from the data source and create the feature vector set.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
    "# the first parameter to the function drop is the name of the column \n",
    "# and the second parameter 'axis' indicates whether we are referring to an index or an actual column name:\n",
    "# 0 or ‘index’, 1 or ‘columns’\n",
    "X = data.drop('SR_FLAG', axis='columns')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# split CAL_DATE to numeric values\n",
    "## SUPER IMPT: This code block only executes once. To reexecute, reset the value of X to default again ##\n",
    "\n",
    "# convert CAL_DATE to datetime type\n",
    "X['CAL_DATE'] = pd.to_datetime(X['CAL_DATE'])\n",
    "\n",
    "# from X, split the date column to individual columns and append them back to X\n",
    "X_ext = pd.DataFrame({\n",
    "    # 'CAL_DATE': X['CAL_DATE'],\n",
    "    'YEAR': X['CAL_DATE'].dt.year,\n",
    "    'MONTH': X['CAL_DATE'].dt.month,\n",
    "    'DAY': X['CAL_DATE'].dt.day,\n",
    "    'DAY_OF_YEAR': X['CAL_DATE'].dt.dayofyear,\n",
    "    'WEEK': X['CAL_DATE'].dt.week,\n",
    "    'WEEK_OF_YEAR': X['CAL_DATE'].dt.weekofyear,\n",
    "    'DAY_OF_WEEK': X['CAL_DATE'].dt.dayofweek,\n",
    "    'WEEKDAY': X['CAL_DATE'].dt.weekday,\n",
    "    'QUARTER': X['CAL_DATE'].dt.quarter\n",
    "}).join(X)\n",
    "# .join(X, lsuffix='_left')\n",
    "# }).join(X, on='CAL_DATE', how='right', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# syntax example for join\n",
    "# df_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# drop the original CAL_DATE column\n",
    "X_ext = X_ext.drop('CAL_DATE', axis='columns')\n",
    "\n",
    "# reassign extended X back to X to overrite\n",
    "X = X_ext\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "## CRFA_C\n",
    "# replace the null values within the column 'CRFA_C' with ASCII for '0' that is NUL.\n",
    "X['CRFA_C'].fillna(chr(0), inplace=True)\n",
    "\n",
    "# convert all the string values to ASCII equivalents\n",
    "X['CRFA_C'] = X['CRFA_C'].apply(lambda x: ord(x))\n",
    "\n",
    "\n",
    "##OUTPROC_FLAG\n",
    "# convert the boolean values True/False to integer 1/0\n",
    "X['OUTPROC_FLAG']=X['OUTPROC_FLAG'].apply(lambda x: np.where(x==False,0,1))\n",
    "\n",
    "## hence the only non-numeric column that is left now is ITEM_NO\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# collect all the numeric features in a temp dataframe\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html\n",
    "X_numeric = X.select_dtypes(include=np.number)\n",
    "\n",
    "# fetch the column names of the numeric columns\n",
    "X_numcols = list(X_numeric)\n",
    "print(X_numcols)\n",
    "# use the apply function again to apply logorithmic converion to all numeric columns\n",
    "# since log of negative values is not defined, we validate and change the function for negative values\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: (np.log(1 + x) if x > 0 else -np.log(1 - x)))\n",
    "# X[X_numcols] = X[X_numcols].apply(lambda x: np.log(1 + x))\n",
    "\n",
    "X[X_numcols] = X[X_numcols].apply(lambda x: ((1+x)/(1+abs(x)))*(np.log(1 + abs(x))))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# Again, scaling only applies to numerical data.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# initializing the scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# define a new dataframe to hold the scaled numeric features\n",
    "X_scaled = pd.DataFrame(data = X)\n",
    "\n",
    "# apply the scaling and populate the data\n",
    "X_scaled[X_numcols] = scaler.fit_transform(X[X_numcols])\n",
    "\n",
    "X_scaled.head()\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##\n",
    "\n",
    "# The 'pandas.get_dummies' is a function to convert string columns into one-hot representation.\n",
    "# Alternative is sklearn.preprocessing.OneHotEncoder()\n",
    "# https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-is-more-efficient\n",
    "\n",
    "# count of features before applying one-hot encoding:\n",
    "print('Count of features before one-hot-encoding: ', len(X_scaled.columns))\n",
    "\n",
    "# applying one-hot-encoding\n",
    "# One-hot encoding is failing the modle fitment\n",
    "X_scaled = pd.get_dummies(data=X_scaled)\n",
    "\n",
    "# count of features after applying one-hot encoding:\n",
    "print('Count of features after one-hot-encoding: ', len(X_scaled.columns))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83 50 51  0 75 68 71 66 69 72 90 70]\n"
     ]
    }
   ],
   "source": [
    "print(X['CRFA_C'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITEM_NO']\n"
     ]
    }
   ],
   "source": [
    "X_nonnum = X.select_dtypes(exclude=np.number)\n",
    "X_nnumcols = list(X_nonnum)\n",
    "print(X_nnumcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
